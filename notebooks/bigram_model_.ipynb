{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATA_DIR = '/content/drive/MyDrive/assignment3/data'\n",
        "except ImportError:\n",
        "    # Not on Colab‚Äîfall back to local folder\n",
        "    DATA_DIR = 'data'\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "print(f\"üè∑Ô∏è Using DATA_DIR = {DATA_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MYwNSJyjoAV",
        "outputId": "55f4a1de-7c5d-48a8-8915-2648738e76c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üè∑Ô∏è Using DATA_DIR = /content/drive/MyDrive/assignment3/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "DATA_DIR   = \"/content/drive/MyDrive/assignment3/data\"\n",
        "tokenized_ds = load_from_disk(f\"{DATA_DIR}/wikitext_tokens\")\n",
        "train_data   = tokenized_ds[\"train\"]\n",
        "valid_data   = tokenized_ds[\"validation\"]\n",
        "test_data    = tokenized_ds[\"test\"]"
      ],
      "metadata": {
        "id": "Xl4L_4M5ImzG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Function to clean tokens: lowercase, remove punctuation, handle special tokens\n",
        "def clean_tokens(tokens):\n",
        "    \"\"\"\n",
        "    Clean a list of tokens by lowercasing, removing punctuation, and handling special tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): List of string tokens (e.g., ['weston', '@-@', 'super'])\n",
        "\n",
        "    Returns:\n",
        "        list: Cleaned list of tokens (e.g., ['weston-super'])\n",
        "    \"\"\"\n",
        "    cleaned = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        token = tokens[i].lower()\n",
        "        if token == '@.@':\n",
        "            cleaned.append('.')  # Replace '@.@' with decimal point\n",
        "        elif token == '@-@':\n",
        "            if i > 0 and i < len(tokens) - 1:\n",
        "                prev_token = cleaned.pop()  # Get the previous token\n",
        "                next_token = tokens[i + 1].lower()  # Get the next token\n",
        "                merged = prev_token + '-' + next_token  # Merge with hyphen\n",
        "                cleaned.append(merged)\n",
        "                i += 1  # Skip the next token since it's merged\n",
        "        elif token == '=' or token in [',', '.', '(', ')', ';', ':', '?', '!', '\"', \"'\"]:\n",
        "            pass  # Remove section markers ('=') and punctuation\n",
        "        else:\n",
        "            cleaned.append(token)\n",
        "        i += 1\n",
        "    return cleaned\n",
        "\n",
        "# Load the dataset (replace with your actual dataset loading code)\n",
        "train_data   = tokenized_ds[\"train\"]\n",
        "valid_data   = tokenized_ds[\"validation\"]\n",
        "test_data    = tokenized_ds[\"test\"]\n",
        "# Clean the data\n",
        "cleaned_test = []\n",
        "\n",
        "\n",
        "\n",
        "for item in test_data:\n",
        "    tokens = item['tokens']\n",
        "    if tokens:\n",
        "        cleaned = clean_tokens(tokens)\n",
        "        if cleaned:\n",
        "            cleaned_test.append(cleaned)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ua9sfggaVqQ4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "unigram_freq = load_counter(\"unigram_freq\")\n",
        "bigram_freq  = load_counter(\"bigram_freq\")\n",
        "trigram_freq = load_counter(\"trigram_freq\")\n",
        "\n",
        "print(len(unigram_freq), len(bigram_freq), len(trigram_freq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFyy85kimdsV",
        "outputId": "df4fbffc-0c2c-48b3-9a82-a3f7e7ba3a89"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "380063 13437868 43001383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary from training data\n",
        "\n",
        "vocab = set(unigram_freq.keys())\n",
        "vocab_size = len(vocab)\n"
      ],
      "metadata": {
        "id": "NmrRxuLonVUM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# Function to compute N-gram probability with Laplace smoothing\n",
        "def ngram_probability(ngram, n, unigram_freq, bigram_freq, trigram_freq, vocab_size):\n",
        "    \"\"\"\n",
        "    Compute the probability of an N-gram using Laplace smoothing.\n",
        "\n",
        "    Args:\n",
        "        ngram (tuple): The N-gram (e.g., ('the', 'sun') for bigram, ('the', 'sun', 'rises') for trigram)\n",
        "        n (int): Order of N-gram (2 for bigram, 3 for trigram)\n",
        "        unigram_freq (Counter): Frequency of unigrams\n",
        "        bigram_freq (Counter): Frequency of bigrams\n",
        "        trigram_freq (Counter): Frequency of trigrams\n",
        "        vocab_size (int): Size of vocabulary\n",
        "\n",
        "    Returns:\n",
        "        float: Smoothed probability of the N-gram\n",
        "    \"\"\"\n",
        "    if n == 2:\n",
        "        prefix = ngram[:-1]  # e.g., ('the',)\n",
        "        word = ngram[-1]    # e.g., 'sun'\n",
        "        prefix_count = unigram_freq[prefix[0]] if prefix[0] in unigram_freq else 0\n",
        "        ngram_count = bigram_freq[ngram] if ngram in bigram_freq else 0\n",
        "        # Laplace smoothing: (count + 1) / (prefix_count + vocab_size)\n",
        "        return (ngram_count + 1) / (prefix_count + vocab_size)\n",
        "    elif n == 3:\n",
        "        prefix = ngram[:-1]  # e.g., ('the', 'sun')\n",
        "        word = ngram[-1]    # e.g., 'rises'\n",
        "        prefix_count = bigram_freq[prefix] if prefix in bigram_freq else 0\n",
        "        ngram_count = trigram_freq[ngram] if ngram in trigram_freq else 0\n",
        "        return (ngram_count + 1) / (prefix_count + vocab_size)\n",
        "    else:\n",
        "        raise ValueError(\"Only bigrams (n=2) and trigrams (n=3) are supported\")\n",
        "\n",
        "# Function to compute perplexity for N-gram model\n",
        "def compute_perplexity(test_data, n, unigram_freq, bigram_freq, trigram_freq, vocab_size):\n",
        "    \"\"\"\n",
        "    Compute perplexity of an N-gram model on test data.\n",
        "\n",
        "    Args:\n",
        "        test_data (list): List of cleaned token lists\n",
        "        n (int): Order of N-gram (2 for bigram, 3 for trigram)\n",
        "        unigram_freq (Counter): Frequency of unigrams\n",
        "        bigram_freq (Counter): Frequency of bigrams\n",
        "        trigram_freq (Counter): Frequency of trigrams\n",
        "        vocab_size (int): Size of vocabulary\n",
        "\n",
        "    Returns:\n",
        "        float: Perplexity score\n",
        "    \"\"\"\n",
        "    log_prob_sum = 0.0\n",
        "    total_words = 0\n",
        "\n",
        "    for sentence in test_data:\n",
        "        if len(sentence) < n:\n",
        "            continue  # Skip sentences too short for N-gram\n",
        "        # Extract N-grams from the sentence\n",
        "        sentence_ngrams = list(ngrams(sentence, n))\n",
        "        for ngram in sentence_ngrams:\n",
        "            prob = ngram_probability(ngram, n, unigram_freq, bigram_freq, trigram_freq, vocab_size)\n",
        "            log_prob_sum += math.log2(prob) if prob > 0 else math.log2(1e-10)  # Avoid log(0)\n",
        "            total_words += 1\n",
        "\n",
        "    if total_words == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    # Perplexity = 2^(-average log probability)\n",
        "    avg_log_prob = log_prob_sum / total_words\n",
        "    perplexity = 2 ** (-avg_log_prob)\n",
        "    return perplexity\n",
        "\n",
        "# Compute perplexity for bigram and trigram models on test data\n",
        "bigram_perplexity = compute_perplexity(cleaned_test, 2, unigram_freq, bigram_freq, trigram_freq, vocab_size)\n",
        "trigram_perplexity = compute_perplexity(cleaned_test, 3, unigram_freq, bigram_freq, trigram_freq, vocab_size)\n",
        "\n",
        "# Output results\n",
        "\n",
        "print(f\"Number of test sentences: {len(cleaned_test)}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Bigram Perplexity: {bigram_perplexity:.2f}\")\n",
        "print(f\"Trigram Perplexity: {trigram_perplexity:.2f}\")\n",
        "\n",
        "# Save frequency dictionaries for further use in N-gram modeling\n",
        "# Example: bigram_freq[('the', 'sun')] gives the count of \"the sun\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFLUGKC2rsNu",
        "outputId": "822ca9cd-c404-437d-c602-8ff6fe74bbcc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test sentences: 175450\n",
            "Vocabulary size: 380063\n",
            "Bigram Perplexity: 3621.65\n",
            "Trigram Perplexity: 36298.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTgzOtVFr_sw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}